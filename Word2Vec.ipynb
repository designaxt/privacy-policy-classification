{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "import count_data as cd\n",
    "import load_data as ld\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import get_tmpfile, common_texts\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APP350数据导出\n",
    "\n",
    "input_dir = r'C:\\python_project\\ML\\original_documents'\n",
    "get_filename = tf.io.gfile.listdir(input_dir)  # 得到csv文件名列表\n",
    "filenames = []\n",
    "path_format = os.path.join(input_dir, '{}')\n",
    "for filename in get_filename:\n",
    "    part_csv = path_format.format(filename)\n",
    "    filenames.append(part_csv)\n",
    "    \n",
    "#  segments[]存储APP350文本\n",
    "\n",
    "segments = []\n",
    "for filename in filenames:\n",
    "    file = open(filename, encoding='ISO-8859-1', errors='ignore')\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "    segments.append(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  加载训练集与测试集\n",
    "(x_train, y_train), (x_test, y_test) = ld.load_data('data/clean_data_0.5_no_repeat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  选择哪些数据放到datasets中用于训练词向量\n",
    "datasets = []\n",
    "for data in x_train:\n",
    "    datasets.append(data)\n",
    "#for data in x_test:\n",
    "#    datasets.append(data)\n",
    "#for data in segments:\n",
    "#    datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  分词\n",
    "datasets = [segment.split() for segment in datasets]\n",
    "\n",
    "# 训练词向量并保存\n",
    "word2vec = gensim.models.word2vec.Word2Vec(\n",
    "    datasets, size=300, window=3, hs=1, sg=0, iter=5,  min_count=1, workers=25)\n",
    "\n",
    "word2vec.save('data/word2vec_300.w2v')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
