{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "import count_data as cd\n",
    "import load_data as ld\n",
    "import sklearn\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import get_tmpfile, common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TaggedDocument = gensim.models.doc2vec.TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APP350数据导出\n",
    "\n",
    "input_dir = r'C:\\python_project\\ML\\original_documents'\n",
    "get_filename = tf.io.gfile.listdir(input_dir)  # 得到csv文件名列表\n",
    "filenames = []\n",
    "path_format = os.path.join(input_dir, '{}')\n",
    "for filename in get_filename:\n",
    "    part_csv = path_format.format(filename)\n",
    "    filenames.append(part_csv)\n",
    "    \n",
    "# segments[]存储APP350文本\n",
    "\n",
    "segments = []\n",
    "for filename in filenames:\n",
    "    file = open(filename, encoding='ISO-8859-1', errors='ignore')\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "    segments.append(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = ld.load_data('data/clean_data_0.5_no_repeat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(str(y_train[2].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for data in x_train:\n",
    "    datasets.append(data)\n",
    "#for data in x_test:\n",
    "#    datasets.append(data)\n",
    "#for data in segments:\n",
    "#    datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  利用tokenizer进行分词去标点等操作\n",
    "tokenizer = Tokenizer(oov_token='<OOV>', lower=True)\n",
    "tokenizer.fit_on_texts(datasets)\n",
    "word_index = tokenizer.word_index\n",
    "datasets = tokenizer.texts_to_sequences(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privacy policy sci news com be committed protect respect your privacy well inform you our policy concern user privacy we have adopt following term please note that these term be subject change any such change will be include on this page\n"
     ]
    }
   ],
   "source": [
    "datasets = tokenizer.sequences_to_texts(datasets)\n",
    "print(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [segment.split() for segment in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelizeReviews(reviews):\n",
    "    labelized = []\n",
    "    for i,v in enumerate(reviews):\n",
    "        labelized.append(TaggedDocument(words=v, tags=['SENT_%s' % i, str(y_train[i].numpy())]))\n",
    "    return labelized\n",
    "\n",
    "datasets = labelizeReviews(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Doc2Vec(min_count=1, window=5, vector_size=100, sample=1e-3, negative=5, workers=3,dm=0) \n",
    "ifmodel_vo=model.build_vocab(datasets)\n",
    "for epoch in range(20): \n",
    "    random.shuffle(datasets)\n",
    "    model.train(datasets,total_examples=len(datasets),epochs=model.epochs)\n",
    "#    model.alpha -= 0.002\n",
    "#    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data/doc2vec+label.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
