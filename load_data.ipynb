{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import count_data as cd\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from gensim import corpora\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url=r'C:\\python_project\\ML\\clean_data\\clean_data_0.5_no_repeat.csv', split_num=0.8):\n",
    "#  加载数据\n",
    "    dataset = []\n",
    "    with open(url, 'r', encoding='gbk') as rf:\n",
    "        reader = csv.reader(rf, dialect=csv.excel)\n",
    "        for row in reader:\n",
    "            dataset.append({'id': row[1], 'text': row[2], 'type': row[4]})\n",
    "#  词性标注方法    \n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "#  制作数据集            \n",
    "    sample = []\n",
    "    label = []\n",
    "    y = []\n",
    "    for data in dataset:\n",
    "        sample.append(data['text'])\n",
    "        y.append(data['type'])\n",
    "    for all_label in y:\n",
    "        all_label = eval(all_label)\n",
    "        label.append(all_label)\n",
    "    label = tf.constant(label)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "#  替换文本中的地址、邮件、电话号码以及数字    \n",
    "    url = r'((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n",
    "    mail = r'([\\w\\d_\\.\\-]+)@([\\w\\d\\-]+)(\\.[\\w\\d\\-]+)*(\\.[\\w\\d]{2,6})'\n",
    "    phone = re.compile(r'''( (\\d{3}|\\(\\d{3}\\))?(\\s|-|\\.)? \\d{3}(\\s|-|\\.)\\d{4}(\\s*(ext|x|ext.)\\s*\\d{2,5})?)''',re.VERBOSE)\n",
    "    num = r'-?[1-9]\\d*|-?([1-9]\\d*\\.\\d*|0\\.\\d*[1-9]\\d*|0?\\.0+|0)$'\n",
    "\n",
    "    sample = [re.sub(url,'TAGURL',text) for text in sample]\n",
    "    sample = [re.sub(mail,'TAGMAIL',text) for text in sample]\n",
    "    sample = [re.sub(phone,'TAGPHONE',text) for text in sample]\n",
    "    sample = [re.sub(num,'TAGNUM',text) for text in sample]\n",
    "#  分词\n",
    "    tokenizer = Tokenizer(oov_token='<OOV>', lower=True)\n",
    "    tokenizer.fit_on_texts(sample)\n",
    "    word_index = tokenizer.word_index\n",
    "    sample = tokenizer.texts_to_sequences(sample)\n",
    "    sample = tokenizer.sequences_to_texts(sample)\n",
    "    \n",
    "    sample = [segment.split() for segment in sample]\n",
    "#  去除停用词    \n",
    "    stoplist = set('for a of the and to'.split()) \n",
    "    sample = [[word for word in segment if word not in stoplist]\n",
    "             for segment in sample]\n",
    "    \n",
    "    sample = [pos_tag(segment) for segment in sample]\n",
    "#  用lemmatize将每个词进行词形还原    \n",
    "    new_datasets = []\n",
    "    for segment in sample:\n",
    "        save = []\n",
    "        for word in segment:\n",
    "            save.append(wnl.lemmatize(word[0], get_wordnet_pos(word[1]) or wordnet.NOUN))\n",
    "        new_datasets.append(' '.join(save))\n",
    "    \n",
    "#  划分训练集与测试集并返回    \n",
    "    x_train = new_datasets[:int(len(new_datasets)*split_num)]\n",
    "    x_test = new_datasets[int(len(new_datasets)*split_num):]\n",
    "    y_train = label[:int(len(label)*split_num)]\n",
    "    y_test = label[int(len(label)*split_num):]\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_multi_class(label, class_number=12):\n",
    "    new_label = list(map(lambda a:[], range(class_number)))\n",
    "    for all_label in label:\n",
    "        for i in range(len(all_label)):\n",
    "            new_label[i].append(all_label[i].numpy())\n",
    "    new_label = tf.constant(new_label)\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始的文本对应的标签为\\[0 1 1 0... ...0 1 0\\]，用于多标签分类\n",
    "\n",
    "将该标签进行转换依次提取每个位置的数字作为每个二分类器的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(text):\n",
    "    #  预处理分词\n",
    "    punctuation = r'!\\\"#$%&()*+,.\\-/:;<=>?@[\\]^_`{|}~'\n",
    "    stoplist = set('for a of the and to'.split())   \n",
    "    wnl = WordNetLemmatizer()\n",
    "    text = [re.sub(r'[{}]+'.format(punctuation),' ',segment) for segment in text]    \n",
    "    texts = [[word for word in segment.lower().split() if word not in stoplist]\n",
    "             for segment in text]\n",
    "    frequency = defaultdict(int)\n",
    "    for word in texts:\n",
    "        for token in word:\n",
    "            frequency[token] += 1\n",
    "    texts = [[token for token in word if frequency[token] > 0]\n",
    "             for word in texts]\n",
    "#    texts = [nltk.pos_tag(text) for text in texts]\n",
    "#    texts = [[wnl.lemmatize(word[0], word[1]) for word in text] for text in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词操作方法，在实际使用中没有用上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
