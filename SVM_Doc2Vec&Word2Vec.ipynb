{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import random\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "import load_data as ld\n",
    "import count_data as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到准确率和召回率\n",
    "def evaluate(actual, pred):\n",
    "    epsilon = 1e-7\n",
    "    m_precision = metrics.precision_score(actual, pred,average='macro')\n",
    "    m_recall = metrics.recall_score(actual,pred,average='macro')\n",
    "    f1 = 2*m_precision*m_recall/(m_precision+m_recall+epsilon)\n",
    "    #print('precision:{0:.3f}'.format(m_precision))\n",
    "    #print('recall:{0:0.3f}'.format(m_recall))\n",
    "    #print('F1:{0:0.3f}'.format(f1))\n",
    "    result = [f1, m_precision, m_recall]\n",
    "    return result\n",
    "\n",
    "#创建svm分类器\n",
    "def train_clf(train_data, train_tags ,kernel='rbf'):\n",
    "    clf = svm.SVC(C=300.0, cache_size=2000, class_weight='balanced', coef0=0.0, decision_function_shape=None, degree=3,\n",
    "                  gamma='auto', kernel=kernel, max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                  tol=0.001, verbose=False)\n",
    "    clf.fit(train_data, np.asarray(train_tags))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  选择模型\n",
    "def get_model(model_type, model_name, embedding_dim=100):\n",
    "    if model_type == 'word2vec':\n",
    "        model = gensim.models.word2vec.Word2Vec.load(\"data/{}.w2v\".format(model_name)).wv\n",
    "    elif model_type == 'doc2vec':\n",
    "        model = gensim.models.doc2vec.Doc2Vec.load(\"data/{}.d2v\".format(model_name))\n",
    "    elif model_type == 'google':\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            '../input/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    elif model_type == 'tfidf':\n",
    "        model = TfidfVectorizer(min_df=5,\n",
    "                             max_df=0.8,\n",
    "                             sublinear_tf=True,\n",
    "                             use_idf=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  加载数据集\n",
    "def prepare_train():\n",
    "    (x_train, y_train), (x_test, y_test) = ld.load_data('data/clean_data_0.5_no_repeat.csv')\n",
    "    y_train = ld.transform_to_multi_class(y_train)\n",
    "    y_test = ld.transform_to_multi_class(y_test)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  特征表示用于输入svm\n",
    "def prepare_vec(model, x_train, x_test, model_name, embedding_dim):\n",
    "    '''word2vec (including google-news pretrained)'''\n",
    "    if model_name == 'word2vec':\n",
    "        max_length = 200\n",
    "        tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts(x_train)\n",
    "        word_index = tokenizer.word_index\n",
    "\n",
    "        x_train = tokenizer.texts_to_sequences(x_train)\n",
    "        x_train = pad_sequences(x_train, padding='post', maxlen=max_length)\n",
    "\n",
    "        x_test = tokenizer.texts_to_sequences(x_test)\n",
    "        x_test = pad_sequences(x_test, padding='post', maxlen=max_length)\n",
    "        embedding_matrix = np.zeros((len(word_index) + 1, model.vector_size))\n",
    "        for word, index in word_index.items():\n",
    "            try:\n",
    "                embedding_vector = model.__getitem__(str(word))\n",
    "                embedding_matrix[int(index)] = embedding_vector\n",
    "            except KeyError:\n",
    "                continue\n",
    "        embedder = tf.keras.layers.Embedding(len(word_index) + 1, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False)\n",
    "        x_train = embedder(x_train)\n",
    "        x_test = embedder(x_test)\n",
    "        x_train = tf.keras.layers.GlobalMaxPooling1D()(x_train)\n",
    "        x_test = tf.keras.layers.GlobalMaxPooling1D()(x_test)\n",
    "    '''doc2vec'''\n",
    "    elif model_name == 'doc2vec':\n",
    "        x_train = [segment.split() for segment in x_train]\n",
    "        x_test = [segment.split() for segment in x_test]\n",
    "        x_train = [model.infer_vector(segment) for segment in x_train]\n",
    "        x_test = [model.infer_vector(segment) for segment in x_test]\n",
    "    '''tf-idf'''\n",
    "    elif model_name == 'tfidf':\n",
    "        x_train = vectorizer.fit_transform(x_train)\n",
    "        x_test = vectorizer.transform(x_test)\n",
    "    return (x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  获取得分\n",
    "def get_score(x_train,y_train,x_test,y_test,kernel='rbf'):\n",
    "    type_dic = {'Introductory/Generic': 0, 'Practice not covered': 1,\n",
    "                'Privacy contact information': 2, 'User Access, Edit and Deletion': 3,\n",
    "                'Data Security': 4, 'International and Specific Audiences': 5,\n",
    "                'Do Not Track': 6, 'User Choice/Control': 7,\n",
    "                'Data Retention': 8, 'Policy Change': 9,\n",
    "                'First Party Collection/Use': 10, 'Third Party Sharing/Collection': 11}\n",
    "    index = 0\n",
    "    table_result = []\n",
    "    clf = []\n",
    "    re = []\n",
    "    for key, value in type_dic.items():\n",
    "        clf.append(train_clf(x_train,y_train[index].numpy(),kernel))\n",
    "        re.append(clf[index].predict(x_test))\n",
    "        result = evaluate(np.asarray(y_test[index]),re[index])\n",
    "        index = index+1\n",
    "        table_result.append([result[0], result[1], result[2]])\n",
    "    print(table_result)\n",
    "    return table_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  计算均值并保存数据，svm实际上跑一次即可\n",
    "def save_result(result,file_name,run_num):\n",
    "    av_F = []\n",
    "    av_p = []\n",
    "    av_r = []\n",
    "    for i in range(12):\n",
    "        f = 0\n",
    "        p = 0\n",
    "        r = 0\n",
    "        for j in range(run_num):\n",
    "            f = f+result[j][i][0]\n",
    "            p = p+result[j][i][1]\n",
    "            r = r+result[j][i][2]\n",
    "        av_F.append(f/run_num)\n",
    "        av_p.append(p/run_num)\n",
    "        av_r.append(r/run_num)\n",
    "    table_result = []\n",
    "    all_F1 = 0\n",
    "    all_P = 0\n",
    "    all_R = 0\n",
    "    for i in range(12):\n",
    "        table_result.append(['/'.join([str('%.2f' % e) for e in [av_F[i],av_p[i],av_r[i]]])])\n",
    "        all_F1 = all_F1+av_F[i]\n",
    "        all_P = all_P+av_p[i]\n",
    "        all_R = all_R+av_r[i]\n",
    "    table_result.append(['/'.join([str('%.2f' % e) for e in [all_F1/12, all_P/12, all_R/12]])])\n",
    "    print(table_result)\n",
    "    with open(r'{}.csv'.format(file_name), 'w', encoding='gbk', newline='') as f:\n",
    "                writer = csv.writer(f, dialect=csv.excel, delimiter=',')\n",
    "                for data in table_result:\n",
    "                    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6860150149288475, 0.6425811573747353, 0.7357462697451904], [0.6493339345069844, 0.6120912547528518, 0.6914024056861673], [0.7887332826470623, 0.7681181690708976, 0.8104855751051004], [0.7298641013044551, 0.6777338122586966, 0.7906823322442824], [0.7390586828205026, 0.6894344290844453, 0.7963808557415863], [0.8601731084592654, 0.8248847926267281, 0.8986157145642425], [0.8883509339683862, 0.998639455782313, 0.8], [0.729939961507733, 0.6931451323624364, 0.7708603191770314], [0.6987526256864274, 0.65276369168357, 0.7517129010695187], [0.7529486033758102, 0.7150558336542164, 0.7950823003454582], [0.7597487930333204, 0.7617362415137946, 0.7577717879604672], [0.7472207638950172, 0.738980350504514, 0.7556471306471306]]\n",
      "[[0.6860150149288475, 0.6425811573747353, 0.7357462697451904], [0.6493339345069844, 0.6120912547528518, 0.6914024056861673], [0.7887332826470623, 0.7681181690708976, 0.8104855751051004], [0.7298641013044551, 0.6777338122586966, 0.7906823322442824], [0.7390586828205026, 0.6894344290844453, 0.7963808557415863], [0.8601731084592654, 0.8248847926267281, 0.8986157145642425], [0.8883509339683862, 0.998639455782313, 0.8], [0.729939961507733, 0.6931451323624364, 0.7708603191770314], [0.6987526256864274, 0.65276369168357, 0.7517129010695187], [0.7529486033758102, 0.7150558336542164, 0.7950823003454582], [0.7597487930333204, 0.7617362415137946, 0.7577717879604672], [0.7472207638950172, 0.738980350504514, 0.7556471306471306]]\n",
      "[[0.6860150149288475, 0.6425811573747353, 0.7357462697451904], [0.6493339345069844, 0.6120912547528518, 0.6914024056861673], [0.7887332826470623, 0.7681181690708976, 0.8104855751051004], [0.7298641013044551, 0.6777338122586966, 0.7906823322442824], [0.7390586828205026, 0.6894344290844453, 0.7963808557415863], [0.8601731084592654, 0.8248847926267281, 0.8986157145642425], [0.8883509339683862, 0.998639455782313, 0.8], [0.729939961507733, 0.6931451323624364, 0.7708603191770314], [0.6987526256864274, 0.65276369168357, 0.7517129010695187], [0.7529486033758102, 0.7150558336542164, 0.7950823003454582], [0.7597487930333204, 0.7617362415137946, 0.7577717879604672], [0.7472207638950172, 0.738980350504514, 0.7556471306471306]]\n",
      "[[0.6860150149288475, 0.6425811573747353, 0.7357462697451904], [0.6493339345069844, 0.6120912547528518, 0.6914024056861673], [0.7887332826470623, 0.7681181690708976, 0.8104855751051004], [0.7298641013044551, 0.6777338122586966, 0.7906823322442824], [0.7390586828205026, 0.6894344290844453, 0.7963808557415863], [0.8601731084592654, 0.8248847926267281, 0.8986157145642425], [0.8883509339683862, 0.998639455782313, 0.8], [0.729939961507733, 0.6931451323624364, 0.7708603191770314], [0.6987526256864274, 0.65276369168357, 0.7517129010695187], [0.7529486033758102, 0.7150558336542164, 0.7950823003454582], [0.7597487930333204, 0.7617362415137946, 0.7577717879604672], [0.7472207638950172, 0.738980350504514, 0.7556471306471306]]\n"
     ]
    }
   ],
   "source": [
    "#  运行并保存数据，逻辑类似于10_run_average\n",
    "result = []\n",
    "run_num = 10\n",
    "kernel = 'rbf'\n",
    "model = get_model(model_type='google',model_name='word2vec')\n",
    "for i in range(run_num):\n",
    "    (x_train, y_train), (x_test, y_test) = prepare_train()\n",
    "    (x_train, x_test) = prepare_vec(model=model,\n",
    "                                    embedding_dim=300, \n",
    "                                    x_train=x_train, \n",
    "                                    x_test=x_test, \n",
    "                                    model_name='word2vec')\n",
    "    result.append(get_score(x_train=x_train,y_train=y_train,x_test=x_test,y_test=y_test,kernel=kernel))\n",
    "save_result(result=result,file_name='svm_google_embed300_len200_10run',run_num=run_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh /root/shutdown.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
